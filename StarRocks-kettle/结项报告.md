# 支持StarRocks Kettle Connector结项报告



## 一、项目信息

### 1.1 项目描述

| **项目名称**   | 支持 StarRocks Kettle Connector                              |
| -------------- | ------------------------------------------------------------ |
| **项目导师**   | **李鹏飞**                                                   |
| **导师邮箱**   | lipengfei@starrocks.com                                      |
| **技术领域**   | **Database、Java**                                           |
| **开源协议**   | **Apache License 2.0**                                       |
| **项目简述**   | **Kettle 是一款开源的 ETL 工具，采用 Java 编写，可以在 Windows、Linux、Unix上 运行，支持低代码、拖拽方式开发 ETL 数据管道，可对接包括传统数据库、文件、大数据平台、接口、流数据等数据源。** |
| **项目要求**   | **本项目为 StarRocks 支持 Kettle connector，支持通过 Kettle 将数据写入到 StarRocks。** |
| **技术要求**   | **熟悉 Java 语言，了解数据库基本操作**                       |
| **项目成果库** | https://github.com/StarRocks/starrocks                       |

### 1.2时间规划

#### 1.2.1 项目开发第一阶段（07月01日-08月15日）

1. 仔细阅读Kettle文档中对于插件中方法重写的要求，以及在插件开发中所要用到的辅助类的使用如行处理、日志处理、错误处理等。
2. 完成Data和Meta类，规划在实现StarRocks数据导入的时候需要用到的参数和Step传输的信息。
3. 完成主要的StarRocks-Kettle-Connector类用于主要实现对上一Step传来行数据的处理，和数据的导入。
4. 完成Dialog类，实现数据导入操作的可视化编辑Step 元数据。

#### 1.2.2 项目开发第二阶段（08月16日-09月1日）

1. 对StarRocks-Kettle-Connector插件进行详细测试。
2. 解决中间发现的问题，思考改进的方式。

#### 1.2.3 项目开发第三阶段（09月1日-09月30日）

1. 将项目交到真实用户进行测试，根据测试反馈完善项目。
2. 项目总结撰写使用说明和结项报告。

## 二、项目介绍



### 2.1 StarRocks

StarRocks 是新一代极速全场景 MPP (Massively Parallel Processing) 数据库。StarRocks 的愿景是能够让用户的数据分析变得更加简单和敏捷。用户无需经过复杂的预处理，就可以用 StarRocks 来支持多种数据分析场景的极速分析。StarRocks 架构简洁，采用了全面向量化引擎，并配备全新设计的 CBO (Cost Based Optimizer) 优化器。StarRocks 能很好地支持实时数据分析，并能实现对实时更新数据的高效查询。StarRocks 兼容 MySQL 协议，支持标准 SQL 语法，易于对接使用，全系统无外部依赖，高可用，易于运维管理。

本项目为了满足更多用户的数据处理需求，需要实现更多种类数据的导入。Kettle融合了众多数据源正好满足项目需求，因此尝试实现Kettle连接StarRocks并将数据导入StarRocks中。

#### 2.1.1 Stream Load数据导入

数据导入是指将原始数据按照业务需求进行清洗、转换、并加载到 StarRocks 中的过程，从而可以在 StarRocks 系统中进行极速统一的数据分析。该项目主要采用的是StarRocks中的Stream Load的导入方式。

Stream Load 是一种基于 HTTP 协议的同步导入方式，支持将本地文件或数据流导入到 StarRocks 中。您提交导入作业以后，StarRocks 会同步地执行导入作业，并返回导入作业的结果信息。

#### 2.1.2 Stream Load事务接口

StarRocks 自 2.4 版本起提供 Stream Load 事务接口，事务接口支持通过兼容 HTTP 协议的工具或语言发起接口请求。该接口提供事务管理、数据写入、事务预提交、事务去重和超时管理等功能。项目将调用该事务接口实现数据的导入功能。

### 2.2 Kettle

> **kettle-PDI**:Pentaho Data Integration

Kettle是一款开源的ETL工具，全称为“Kettle ETL”。ETL是指抽取（Extract）、转换（Transform）和加载（Load），是数据仓库建设中的一个重要环节。 Kettle提供了一套完整的ETL工作流程，包括数据抽取、数据转换和数据加载等步骤，支持多种数据源的连接和处理，如关系型数据库、NoSQL数据库、大数据存储、文件格式、Web服务等。

Kettle提供了可视化的图形界面，用户可以通过拖拽组件、配置参数等方式来构建数据处理流程，从而实现数据的抽取、清洗、转换和加载等操作。 Kettle提供了丰富的组件库，包括输入组件、输出组件、转换组件、过滤器、连接器等，用户可以根据自己的需求来选择合适的组件进行组合。

Kettle还支持多种数据处理方式，如增量抽取、全量抽取、数据合并、数据过滤、数据转换、数据分割、数据聚合等。 同时，Kettle还支持任务调度和监控，用户可以通过设置定时任务或事件触发任务等方式来执行数据处理任务，并可随时查看任务执行情况和日志信息。

#### 2.2.1 Kettle特点

* 易用性： 有可视化设计器进行可视化操作，使用简单。
* 功能强大：不仅能进行数据传输，能同时进行数据清洗转换等操作。
* 支持多种源：支持各种数据库、FTP、文件、rest接口、Hadoop集群等源。
* 部署方便：独立部署，不依赖第三方产品。
* 适用场景： 数据量及增量不大，业务规则变化较快，要求可视化操作，对技术人员的技术门槛要求低。

#### 2.2.2 Kettle基本概念

![](E:\javaProjects\StarRocks-kettle\StarRocks-kettle\image\ospp01.png)

##### **Kettle组成**

* Spoon：图形化工具，用于快速设计和维护复杂的ETL工作流。
* Kitchen：运行作业的命令行工具。
* Pan：运行转换的命令行工具。
* Carte：轻量级（大概1MB）Web服务器，用来远程执行转换或作业。一个运行有Carte进程的机器可以作为从服务器，从服务器是Kettle集群的一部分。

##### 功能概念

*  **Transformation：**Transformation（转换）是由一系列被称之为step（步骤）的逻辑工作的网络。转换本质上是数据流。
   * 转换的两个相关的主要组成部分是step（步骤）和hops（节点连接）。
   * 转换文件的扩展名是.ktr。
*  **Steps：**　Steps（步骤）是转换的建筑模块，比如一个文本文件输入或者一个表输出就是一个步骤。在PDI中有140多个步骤，它们按不同功能进行分类，比如输入类、输出类、脚本类等。
*  **Hops：**　Hops（节点连接）是数据的通道，用于连接两个步骤，使得元数据从一个步骤传递到另一个步骤。
*  **Jobs：**Jobs（工作）是基于工作流模型的，协调数据源、执行过程和相关依赖性的ETL活动。
   * 工作由工作节点连接、工作实体和工作设置组成。
   * 工作文件的扩展名是.kjb。

#### 2.2.3 Step之间交互通信

##### RowSet

`RowSet`的实现类，负责步骤之间的相互通信，`rowset`对象即是前一个`step`的成员也是后一个`step`的成员，访问是线程安全的。

![](image/ospp06.jpg)

`RowSet`类中包含源`step`，目标`step`和由源向目标发送的一个`rowMeta`和一组`data`。其中`data`数据是以行为单位的队列（queArray）。一个`RowSet`作为此源`ste`p的`outputrowsets`的一部分。同时作为目标`step`的`inputRowsets`一部分。**源`Step`每次向队列中写一行数据，目标`step`每次从队列中读取一行数据**。

##### 行元数据

所有的`data`均擦除为`object`对象。步骤与步骤之间以行为单位进行处理，自然需要知道每行的结构，即行元数据。行元数据至少需要包括类型、名称，当然还可能包括字段长度、精度等常见内容。

行元数据不仅在执行的时候需要，而且在转换设置的时候同样需要。每个步骤的行元数据都会保存在`.ktr`文件或者数据库里面，所以可以根据步骤名称从`TransMeta`对象中获取行元数据。

行元数据的`UML类`图结构如下所示，主要有单元格元数据组成行元数据。在现有的版本中，支持的数据类型有`String、Date、BigNumber、Boolean、SerializableType、Binary、Integer、Numberic`。

![](image/ospp07.jpg)

#### 2.2.4 Transformation转换机制介绍

StarRocks Kettle Connector主要实现的是一个Kettle的Transformation插件，通过对该插件的配置和调用来实现数据的导入，该部分将详细介绍转换机制的执行流程。

##### Trans执行

trans类的执行有`execute()`负责，主要包含两个步骤：转换执行前的准备工作和所有线程的开启。Trans每一个步骤都会对应一个独立的线程，线程之间通过`RowSet`进行通信交互。

~~~java
public void execute(String[] arguments) throws KettleException {
    prepareExecution(arguments);
    startThreads();
}
~~~

##### 执行准备(`prepareExecution`)

该步骤，主要完成对通信类的初始化，对步骤的包装初始化。最后启动各个步骤初始化线程，即调用各个步骤的`init()`方法。准备结束之后，步骤之间的通信机制完成了，各个步骤的初始化工作也完成了。具体的流程如下所示：

![](image/ospp03.jpg)

##### 转换处理执行(`startThreads`)

Trans转换执行引擎类，通过`startThreads()`启动步骤线程。为所有步骤添加监听器，在开启监听进程对所有线程进行监听。具体的步骤如下所示

![](image/ospp04.jpg)

##### 步骤执行过程

实现`StepInterface`的不同的`step`各个功能个不一样，但是它们之间也有一定的规律性。下面以`csv输入`和`StarRocks Kettle Connector`两个`step` 为例。

![](image/ospp05.jpg)

BaseStep封装了`getRow()`和`putRow()`方法，从上一个步骤获取数据和将数据输入到下一个步骤。基类`BaseStep`采取了统一的处理方式，调用子类`processRow`以行为单位处理。

`processRow`通用过程是：调用基类`BaseStep` 的`getRow( )`得到数据，对一行数据处理，处理之后调用基类`putRow( )`方法数据保存至`outputRowSets`（即`next step`的`inputRowSets`）.

### 2.3 StarRocks Kettle Connector

##### Why

目前，StarRocks兼容并支持DataX、Flink以及Spark这三种高效的数据处理框架，从而实现数据的有效写入到StarRocks中。然而，需要注意的是，这三种框架的操作均基于命令行或代码，这可能对非技术人员构成一定的挑战。因此，为了增加易用性并优化用户体验，我们正在扩展StarRocks的Kettle Connector，以实现直观的、可视化的数据导入操作，使得无论技术背景如何，用户都能够方便快捷地进行数据导入。

Kettle是一个流行的ETL工具，它提供了一种可视化的图形界面，用户可以通过拖拽组件、配置参数等方式来构建数据处理流程。这种直观的操作方式大大简化了数据处理和导入的过程，使得用户可以更加便捷地处理数据。此外，Kettle还提供了丰富的操作组件库，用户可以根据自己的需求选择合适的组件，实现各种复杂的数据处理任务。

通过扩展StarRocks对Kettle的连接功能，用户不仅可以实现更方便的数据导入，还可以利用Kettle的操作组件库，提供更便捷、更灵活的数据处理和导入方式。用户可以更加方便地从各种数据源读取数据，然后通过Kettle的数据处理流程，将处理后的数据导入到StarRocks。

##### What

StarRocks Kettle Connector实现了Kettle的一个插件，它用于在StarRocks和Kettle之间建立连接，以实现众多数据源数据向StarRocks导入和ETL（Extract, Transform, Load）功能。通过此插件，可以将Kettle的强大数据处理和转换功能与StarRocks的高性能数据存储和分析能力相结合，从而实现更加灵活和高效的数据处理流程。

使用StarRocks Kettle Connector的场景包括：

1. 数据集成：当需要从不同的数据源中抽取数据，进行数据清洗和转换，最后将数据加载到StarRocks中进行分析和查询时，可以使用此功能来实现数据集成和ETL。
2. 复杂数据处理：当数据处理流程比较复杂，需要多个数据转换步骤和数据源连接时，可以利用Kettle的可视化界面来设计和配置ETL工作流程，最后将数据记载到StarRocks，提高开发效率和灵活性。
3. 数据转换和整合：当需要对原始数据进行复杂的转换和整合，以满足特定的数据分析和查询需求时，可以使用Kettle的强大数据转换功能来实现。

通过StarRocks Kettle Connector，用户可以获得以下好处：

1. 便捷性：利用Kettle的可视化界面，可以以图形化方式设计和配置复杂的ETL工作流程，简化了从不同数据源向StarRocks的数据加载过程，降低学习成本。
2. 灵活性：通过与Kettle的连接，扩展了StarRocks的数据处理能力，使得用户可以根据自己的需求选择适合的工具和方式来进行数据处理。
3. 高性能：StarRocks作为一个高性能的数据存储和分析引擎，与Kettle的连接可以将高效的数据加载与复杂的数据转换和整合相结合，从而提高数据处理的性能。

## 三、 项目方案



## 四、 项目功能



## 五、 项目测试