# 支持StarRocks Kettle Connector结项报告



## 一、项目名称

| **项目名称**   | 支持 StarRocks Kettle Connector                              |
| -------------- | ------------------------------------------------------------ |
| **项目导师**   | **李鹏飞**                                                   |
| **导师邮箱**   | lipengfei@starrocks.com                                      |
| **技术领域**   | **Database、Java**                                           |
| **开源协议**   | **Apache License 2.0**                                       |
| **项目简述**   | **Kettle 是一款开源的 ETL 工具，采用 Java 编写，可以在 Windows、Linux、Unix上 运行，支持低代码、拖拽方式开发 ETL 数据管道，可对接包括传统数据库、文件、大数据平台、接口、流数据等数据源。** |
| **项目要求**   | **本项目为 StarRocks 支持 Kettle connector，支持通过 Kettle 将数据写入到 StarRocks。** |
| **技术要求**   | **熟悉 Java 语言，了解数据库基本操作**                       |
| **项目成果库** | https://github.com/StarRocks/starrocks                       |



## 二、项目介绍

### 2.1 StarRocks

StarRocks 是新一代极速全场景 MPP (Massively Parallel Processing) 数据库。StarRocks 的愿景是能够让用户的数据分析变得更加简单和敏捷。用户无需经过复杂的预处理，就可以用 StarRocks 来支持多种数据分析场景的极速分析。StarRocks 架构简洁，采用了全面向量化引擎，并配备全新设计的 CBO (Cost Based Optimizer) 优化器。StarRocks 能很好地支持实时数据分析，并能实现对实时更新数据的高效查询。StarRocks 兼容 MySQL 协议，支持标准 SQL 语法，易于对接使用，全系统无外部依赖，高可用，易于运维管理。

本项目为了满足更多用户的数据处理需求，需要实现更多种类数据的导入。Kettle融合了众多数据源正好满足项目需求，因此尝试实现Kettle连接StarRocks并将数据导入StarRocks中。

#### 2.1.1 Stream Load数据导入

数据导入是指将原始数据按照业务需求进行清洗、转换、并加载到 StarRocks 中的过程，从而可以在 StarRocks 系统中进行极速统一的数据分析。该项目主要采用的是StarRocks中的Stream Load的导入方式。

Stream Load 是一种基于 HTTP 协议的同步导入方式，支持将本地文件或数据流导入到 StarRocks 中。您提交导入作业以后，StarRocks 会同步地执行导入作业，并返回导入作业的结果信息。

#### 2.1.2 Stream Load事务接口

StarRocks 自 2.4 版本起提供 Stream Load 事务接口，事务接口支持通过兼容 HTTP 协议的工具或语言发起接口请求。该接口提供事务管理、数据写入、事务预提交、事务去重和超时管理等功能。项目将调用该事务接口实现数据的导入功能。

### 2.2 Kettle

> **kettle-PDI**:Pentaho Data Integration

Kettle是一款开源的ETL工具，全称为“Kettle ETL”。ETL是指抽取（Extract）、转换（Transform）和加载（Load），是数据仓库建设中的一个重要环节。 Kettle提供了一套完整的ETL工作流程，包括数据抽取、数据转换和数据加载等步骤，支持多种数据源的连接和处理，如关系型数据库、NoSQL数据库、大数据存储、文件格式、Web服务等。

Kettle提供了可视化的图形界面，用户可以通过拖拽组件、配置参数等方式来构建数据处理流程，从而实现数据的抽取、清洗、转换和加载等操作。 Kettle提供了丰富的组件库，包括输入组件、输出组件、转换组件、过滤器、连接器等，用户可以根据自己的需求来选择合适的组件进行组合。

Kettle还支持多种数据处理方式，如增量抽取、全量抽取、数据合并、数据过滤、数据转换、数据分割、数据聚合等。 同时，Kettle还支持任务调度和监控，用户可以通过设置定时任务或事件触发任务等方式来执行数据处理任务，并可随时查看任务执行情况和日志信息。

#### 2.2.1 Kettle特点

* 易用性： 有可视化设计器进行可视化操作，使用简单。
* 功能强大：不仅能进行数据传输，能同时进行数据清洗转换等操作。
* 支持多种源：支持各种数据库、FTP、文件、rest接口、Hadoop集群等源。
* 部署方便：独立部署，不依赖第三方产品。
* 适用场景： 数据量及增量不大，业务规则变化较快，要求可视化操作，对技术人员的技术门槛要求低。

#### 2.2.2 Kettle基本概念

![](E:\javaProjects\StarRocks-kettle\StarRocks-kettle\image\ospp01.png)

##### **Kettle组成**

* Spoon：图形化工具，用于快速设计和维护复杂的ETL工作流。
* Kitchen：运行作业的命令行工具。
* Pan：运行转换的命令行工具。
* Carte：轻量级（大概1MB）Web服务器，用来远程执行转换或作业。一个运行有Carte进程的机器可以作为从服务器，从服务器是Kettle集群的一部分。

##### 功能概念

*  **Transformation：**Transformation（转换）是由一系列被称之为step（步骤）的逻辑工作的网络。转换本质上是数据流。
   * 转换的两个相关的主要组成部分是step（步骤）和hops（节点连接）。
   * 转换文件的扩展名是.ktr。
*  **Steps：**　Steps（步骤）是转换的建筑模块，比如一个文本文件输入或者一个表输出就是一个步骤。在PDI中有140多个步骤，它们按不同功能进行分类，比如输入类、输出类、脚本类等。
*  **Hops：**　Hops（节点连接）是数据的通道，用于连接两个步骤，使得元数据从一个步骤传递到另一个步骤。
*  **Jobs：**Jobs（工作）是基于工作流模型的，协调数据源、执行过程和相关依赖性的ETL活动。
   * 工作由工作节点连接、工作实体和工作设置组成。
   * 工作文件的扩展名是.kjb。

#### 2.2.3 Step之间交互通信

##### RowSet

`RowSet`的实现类，负责步骤之间的相互通信，`rowset`对象即是前一个`step`的成员也是后一个`step`的成员，访问是线程安全的。

![](image/ospp06.jpg)

`RowSet`类中包含源`step`，目标`step`和由源向目标发送的一个`rowMeta`和一组`data`。其中`data`数据是以行为单位的队列（queArray）。一个`RowSet`作为此源`ste`p的`outputrowsets`的一部分。同时作为目标`step`的`inputRowsets`一部分。**源`Step`每次向队列中写一行数据，目标`step`每次从队列中读取一行数据**。

##### 行元数据

所有的`data`均擦除为`object`对象。步骤与步骤之间以行为单位进行处理，自然需要知道每行的结构，即行元数据。行元数据至少需要包括类型、名称，当然还可能包括字段长度、精度等常见内容。

行元数据不仅在执行的时候需要，而且在转换设置的时候同样需要。每个步骤的行元数据都会保存在`.ktr`文件或者数据库里面，所以可以根据步骤名称从`TransMeta`对象中获取行元数据。

行元数据的`UML类`图结构如下所示，主要有单元格元数据组成行元数据。在现有的版本中，支持的数据类型有`String、Date、BigNumber、Boolean、SerializableType、Binary、Integer、Numberic`。

![](image/ospp07.jpg)

#### 2.2.4 Transformation转换机制介绍

StarRocks Kettle Connector主要实现的是一个Kettle的Transformation插件，通过对该插件的配置和调用来实现数据的导入，该部分将详细介绍转换机制的执行流程。

##### Trans执行

trans类的执行有`execute()`负责，主要包含两个步骤：转换执行前的准备工作和所有线程的开启。Trans每一个步骤都会对应一个独立的线程，线程之间通过`RowSet`进行通信交互。

~~~java
public void execute(String[] arguments) throws KettleException {
    prepareExecution(arguments);
    startThreads();
}
~~~

##### 执行准备(`prepareExecution`)

该步骤，主要完成对通信类的初始化，对步骤的包装初始化。最后启动各个步骤初始化线程，即调用各个步骤的`init()`方法。准备结束之后，步骤之间的通信机制完成了，各个步骤的初始化工作也完成了。具体的流程如下所示：

![](image/ospp03.jpg)

##### 转换处理执行(`startThreads`)

Trans转换执行引擎类，通过`startThreads()`启动步骤线程。为所有步骤添加监听器，在开启监听进程对所有线程进行监听。具体的步骤如下所示

![](image/ospp04.jpg)

##### 步骤执行过程

实现`StepInterface`的不同的`step`各个功能个不一样，但是它们之间也有一定的规律性。下面以`csv输入`和`StarRocks Kettle Connector`两个`step` 为例。

![](image/ospp05.jpg)

BaseStep封装了`getRow()`和`putRow()`方法，从上一个步骤获取数据和将数据输入到下一个步骤。基类`BaseStep`采取了统一的处理方式，调用子类`processRow`以行为单位处理。

`processRow`通用过程是：调用基类`BaseStep` 的`getRow( )`得到数据，对一行数据处理，处理之后调用基类`putRow( )`方法数据保存至`outputRowSets`（即`next step`的`inputRowSets`）.

### 2.3 StarRocks Kettle Connector

